import os, re
import pandas as pd
import numpy as np
from datetime import datetime, time
import streamlit as st
from typing import Optional

# å°å…¥ç®¡ç†åŠŸèƒ½æ¨¡çµ„
try:
    from admin_features import show_admin_dashboard
    ADMIN_FEATURES_AVAILABLE = True
except ImportError:
    ADMIN_FEATURES_AVAILABLE = False

# ====== åŸºæœ¬è¨­å®š ======
st.set_page_config(page_title="æ„›çˆ¾é”ç¯€ç›®è¡¨äº’å‹•å¹³å°", layout="wide")
DEFAULT_DIR = "./"
DEFAULT_SCHEDULE = os.path.join(DEFAULT_DIR, "program_schedule_extracted.csv")
DEFAULT_RATINGS  = os.path.join(DEFAULT_DIR, "integrated_program_ratings_cleaned.csv")

# ====== è¼”åŠ©å‡½æ•¸ ======
def parse_time_str(x):
    try:
        if pd.isna(x): return None
        s = str(x).strip()
        if not s: return None
        m = re.match(r"^(\d{1,2}):(\d{1,2})", s)
        if m:
            hh, mm = int(m.group(1)), int(m.group(2))
            if 0 <= hh <= 23 and 0 <= mm <= 59:
                return time(hh, mm)
        ts = pd.to_datetime(s, errors="coerce")
        return ts.time() if not pd.isna(ts) else None
    except:
        return None

def hour_bucket_from_time(t):
    if not isinstance(t, time): return "other"
    h = t.hour
    if 8<=h<10: return "08-10"
    if 10<=h<12: return "10-12"
    if 12<=h<14: return "12-14"
    if 14<=h<17: return "14-17"
    if 17<=h<19: return "17-19"
    if 19<=h<21: return "19-21"
    if 20<=h<22: return "20-22"
    return "other"

def _rename_first_match(df: pd.DataFrame, candidates, target):
    """
    åœ¨ df.columns ä¸­æ‰¾ç¬¬ä¸€å€‹å­˜åœ¨çš„ candidates æ¬„ä½ï¼ˆå¤§å°å¯«ä¸æ•æ„Ÿï¼‰ï¼Œrename æˆ targetã€‚
    è‹¥æ‰¾ä¸åˆ°ï¼Œä¸å‹•ä½œã€‚
    """
    lower_map = {c.lower(): c for c in df.columns}
    for cand in candidates:
        key = cand.lower()
        if key in lower_map:
            real = lower_map[key]
            if real != target:
                df.rename(columns={real: target}, inplace=True)
            return

def normalize_series(s: str) -> str:
    s = str(s).strip()
    s = s.replace("ï¼ƒ", "#").replace("ï¼š", ":")
    s = re.sub(r"[#ï¼ƒ]\s*\d+$", "", s)                 # å»æ‰ #7 / ï¼ƒ12 ç­‰å°¾ç¢¼
    s = re.sub(r"(ç¬¬\s*\d+\s*(é›†|å­£|éƒ¨))$", "", s)      # å»æ‰ ç¬¬7é›† / ç¬¬2å­£ / ç¬¬3éƒ¨
    s = re.sub(r"\s+$", "", s)
    s = re.sub(r"[#ï¼ƒ]$", "", s)                       # å»æ‰å°¾ç«¯å­¤ç«‹çš„ #
    return s

@st.cache_data(show_spinner=False)
def load_schedule(path: str) -> pd.DataFrame:
    df = pd.read_csv(path, encoding="utf-8-sig")

    # 1) å…ˆæŠŠå¸¸è¦‹æ¬„ä½åç¨±å°é½Šï¼ˆå¤§å°å¯«/ä¸­è‹±ï¼‰
    _rename_first_match(df, ["date", "æ—¥æœŸ", "æ’­å‡ºæ—¥æœŸ", "æ’­æ˜ æ—¥æœŸ"], "date")
    _rename_first_match(df, ["start_time", "time", "é–‹å§‹æ™‚é–“", "æ’­æ”¾é–‹å§‹æ™‚é–“", "æ’­å‡ºæ™‚é–“"], "start_time")
    _rename_first_match(df, ["program_title", "program", "ç¯€ç›®", "ç¯€ç›®åç¨±"], "program_title")
    _rename_first_match(df, ["weekday_idx", "weekday_index"], "weekday_idx")
    _rename_first_match(df, ["weekday_name", "weekday"], "weekday_name")
    _rename_first_match(df, ["hour_bucket", "æ™‚æ®µ"], "hour_bucket")
    _rename_first_match(df, ["source_sheet", "sheet"], "source_sheet")

    # 2) æ¬„ä½å­˜åœ¨æ€§ä¿åº•ï¼šæ²’æœ‰å°±å…ˆå»ºèµ·ä¾†ï¼Œé¿å… KeyError
    if "date" not in df.columns: df["date"] = pd.NaT
    if "start_time" not in df.columns: df["start_time"] = None
    if "program_title" not in df.columns:
        # è‹¥åŸå§‹åªæœ‰ seriesï¼Œå°±é€€è€Œç”¨ä¹‹
        _rename_first_match(df, ["series", "Cleaned_Series_Name"], "program_title")
        if "program_title" not in df.columns:
            df["program_title"] = None
    for c in ["weekday_idx", "weekday_name", "hour_bucket", "source_sheet"]:
        if c not in df.columns: df[c] = None

    # 3) è½‰å‹èˆ‡è¡ç”Ÿ
    df["date"] = pd.to_datetime(df["date"], errors="coerce")
    df["start_time"] = df["start_time"].apply(parse_time_str)
    if "start_hour" not in df.columns or df["start_hour"].isna().all():
        df["start_hour"] = df["start_time"].apply(lambda x: x.hour if isinstance(x, time) else None)
    if "hour_bucket" not in df.columns or df["hour_bucket"].isna().all():
        df["hour_bucket"] = df["start_time"].apply(hour_bucket_from_time)

    # 4) å»º series ä¾›ç›¸ä¼¼åº¦ä½¿ç”¨
    df["series"] = df["program_title"].astype(str).str.replace(r"[ç¬¬é›†å­£éƒ¨\s\d]+$", "", regex=True)
    df["series"] = df["series"].map(normalize_series)
    return df


@st.cache_data(show_spinner=False)
def load_ratings(path: str) -> Optional[pd.DataFrame]:
    if not os.path.exists(path):
        return None
    try:
        r = pd.read_csv(path, encoding="utf-8-sig")

        # (1) æŠŠç¯€ç›®åç¨±æ¬„ä½çµ±ä¸€æˆ series
        if "Cleaned_Series_Name" in r: 
            r = r.rename(columns={"Cleaned_Series_Name":"series"})
        elif "program_title_clean" in r: 
            r = r.rename(columns={"program_title_clean":"series"})
        elif "Program" in r: 
            r = r.rename(columns={"Program":"series"})

        # (2) æŠŠæ”¶è¦–æ¬„ä½çµ±ä¸€æˆ Ratingï¼ˆè‹¥æª”æ¡ˆå·²æ˜¯èšåˆçµæœå¯è·³éï¼‰
        if "Rating" not in r:
            for cand in ["rating","å¹³å‡æ”¶è¦–ç‡","æ”¶è¦–ç‡"]:
                if cand in r.columns:
                    r = r.rename(columns={cand:"Rating"})
                    break

        # â˜…â˜…â˜… å°±åŠ åœ¨é€™ä¸€è¡Œï¼šrename å®Œæˆä¹‹å¾Œã€groupby ä¹‹å‰
        if "series" in r.columns:
            r["series"] = r["series"].astype(str).map(normalize_series)

        # (3) è‹¥æ˜¯æ˜ç´°æª”æ¡ˆ â†’ èšåˆæˆæ¯ç³»åˆ—çš„çµ±è¨ˆ
        if "series" in r.columns and "Rating" in r.columns:
            r["Rating"] = pd.to_numeric(r["Rating"], errors="coerce")
            r = r.dropna(subset=["series"])
            return r.groupby("series", as_index=False).agg(
                rating_mean=("Rating","mean"),
                rating_median=("Rating","median"),
                rating_count=("Rating","count")
            )

        # (4) è‹¥ä½ çš„ CSV å·²ç¶“æ˜¯èšåˆè¡¨ï¼ˆå« rating_mean/median/countï¼‰ï¼Œç›´æ¥å›å‚³ä¹Ÿè¡Œ
        if {"series","rating_mean","rating_median","rating_count"}.issubset(set(r.columns)):
            return r[["series","rating_mean","rating_median","rating_count"]]

    except Exception as e:
        try:
            st.warning(f"è®€å– ratings å¤±æ•—ï¼š{e}")
        except:
            print(f"[WARN] è®€å– ratings å¤±æ•—ï¼š{e}")
    return None

@st.cache_data(show_spinner=False)
def build_slot_pop(SDF: pd.DataFrame, RDF: Optional[pd.DataFrame], days:int=60) -> pd.DataFrame:
    cutoff = SDF["date"].max() - pd.Timedelta(days=days)
    hist = SDF[SDF["date"] >= cutoff].copy()
    if RDF is not None:
        hist = hist.merge(RDF, on="series", how="left")
        hist["slot_value"] = hist["rating_mean"].fillna(0)
    else:
        hist["slot_value"] = 1.0
    grp = (hist.groupby(["weekday_idx","hour_bucket","series"], as_index=False)
               .agg(avg_slot_value=("slot_value","mean"), n=("slot_value","count")))
    grp["slot_key"] = grp["weekday_idx"].astype(str)+"|"+grp["hour_bucket"].astype(str)
    grp["slot_pop_score"] = grp.groupby("slot_key")["avg_slot_value"].rank(pct=True)
    return grp[["weekday_idx","hour_bucket","series","slot_pop_score","n"]]

@st.cache_data(show_spinner=False)
def build_trend(SDF: pd.DataFrame, RDF: Optional[pd.DataFrame]) -> pd.DataFrame:
    daily = SDF.groupby(["series","date"], as_index=False).agg(cnt=("series","count"))
    if RDF is not None:
        rr = SDF.merge(RDF, on="series", how="left")
        daily_r = rr.groupby(["series","date"], as_index=False).agg(daily_value=("rating_mean","mean"))
    else:
        daily_r = daily.rename(columns={"cnt":"daily_value"})
    daily_r = daily_r.sort_values(["series","date"])
    daily_r["ma7"]  = daily_r.groupby("series")["daily_value"].transform(lambda s: s.rolling(7, min_periods=3).mean())
    daily_r["ma30"] = daily_r.groupby("series")["daily_value"].transform(lambda s: s.rolling(30, min_periods=5).mean())
    daily_r["trend"] = daily_r["ma7"] - daily_r["ma30"]
    last = daily_r.groupby("series", as_index=False).agg(trend=("trend","last"))
    last["trend_z"] = (last["trend"] - last["trend"].mean()) / (last["trend"].std() + 1e-6)
    return last[["series","trend_z"]]

# === bucket ç›¸é„°è¡¨ï¼ˆç”¨æ–¼ Fallback L1ï¼‰===
BUCKET_NEIGHBORS = {
    "08-10": ["10-12"],
    "10-12": ["08-10", "12-14"],
    "12-14": ["10-12", "14-17"],
    "14-17": ["12-14", "17-19"],
    "17-19": ["14-17", "19-21"],
    "19-21": ["17-19", "20-22"],
    "20-22": ["19-21"],
    "other": []  # other ä¸æŒ‡å®šç›¸é„°æ¡¶
}

def get_slot_candidates(SLOT_POP: pd.DataFrame, wkd: int, hb: str) -> pd.DataFrame:
    """
    ä¾ç…§ Fallback éšæ¢¯å›å‚³ slot-pop å€™é¸ï¼š
    L0: åŒ weekday + åŒ bucket
    L1: åŒ weekday + ç›¸é„° bucketï¼ˆä¾ BUCKET_NEIGHBORSï¼‰
    L2: åŒ weekday + å…¨æ¡¶
    L3: å…¨ weekday + åŒ bucket
    L4: å…¨åŸŸï¼ˆæ‰€æœ‰è³‡æ–™ï¼‰
    """
    base_cols = ["series", "slot_pop_score"]
    take = lambda mask: SLOT_POP.loc[mask, base_cols]

    # L0: åŒ weekday + åŒ bucket
    c = take((SLOT_POP["weekday_idx"] == wkd) & (SLOT_POP["hour_bucket"] == hb))
    if not c.empty:
        return c

    # L1: åŒ weekday + ç›¸é„° bucket
    for nb in BUCKET_NEIGHBORS.get(hb, []):
        c = take((SLOT_POP["weekday_idx"] == wkd) & (SLOT_POP["hour_bucket"] == nb))
        if not c.empty:
            return c

    # L2: åŒ weekday + å…¨æ¡¶
    c = take(SLOT_POP["weekday_idx"] == wkd)
    if not c.empty:
        return c

    # L3: å…¨ weekday + åŒ bucket
    c = take(SLOT_POP["hour_bucket"] == hb)
    if not c.empty:
        return c

    # L4: å…¨åŸŸç†±é–€
    return SLOT_POP[base_cols]

# å…§å®¹ç›¸ä¼¼ï¼ˆå„ªå…ˆ jiebaï¼Œå¤±æ•—æ”¹ char ngramï¼‰
def build_sim_index(SDF: pd.DataFrame):
    try:
        import jieba
        from sklearn.feature_extraction.text import TfidfVectorizer
        def tok(t): return list(jieba.cut(t))
        vec = TfidfVectorizer(tokenizer=tok, max_features=5000)
    except Exception:
        from sklearn.feature_extraction.text import TfidfVectorizer
        vec = TfidfVectorizer(analyzer="char", ngram_range=(2,3), max_features=8000)
    catalog = SDF.groupby("series", as_index=False).agg(n=("series","count"))
    X = vec.fit_transform(catalog["series"].fillna(""))
    return vec, X, catalog

def similar_series(seed, vec, X, catalog, topk=20):
    from sklearn.metrics.pairwise import cosine_similarity
    ser = catalog["series"].tolist()
    idx = {s:i for i,s in enumerate(ser)}
    if seed not in idx: return []
    i = idx[seed]
    sims = cosine_similarity(X[i], X).ravel()
    ords = sims.argsort()[::-1]
    out = []
    for j in ords[:topk+1]:
        s = ser[j]
        if s == seed: continue
        out.append((s, float(sims[j])))
        if len(out) >= topk: break
    return out

def hour_bucket_by_hour(h):
    if 8<=h<10: return "08-10"
    if 10<=h<12: return "10-12"
    if 12<=h<14: return "12-14"
    if 14<=h<17: return "14-17"
    if 17<=h<19: return "17-19"
    if 19<=h<21: return "19-21"
    if 20<=h<22: return "20-22"
    return "other"

# ================== æ©Ÿå™¨å­¸ç¿’ï¼šå°±åœ°å­¸å‡ºæ¬Šé‡ï¼ˆåœ¨ç•¶å‰å€™é¸ä¸Šï¼‰ ==================
def _safe_target_from_cand(cand: pd.DataFrame) -> pd.Series:
    """å„ªå…ˆç”¨ rating_mean ä½œç‚ºå­¸ç¿’ç›®æ¨™ï¼›æ²’æœ‰å°±ç”¨ freq æ­£è¦åŒ–ã€‚"""
    if "rating_mean" in cand.columns and cand["rating_mean"].notna().any():
        y = cand["rating_mean"].copy()
        # é¿å…å°ºåº¦å•é¡Œï¼Œåš 0~1 normalize
        y = (y - y.min()) / (y.max() - y.min() + 1e-9)
        return y
    # fallbackï¼šç”¨é »æ¬¡ä»£è¡¨å—æ­¡è¿ç¨‹åº¦
    if "freq" in cand.columns:
        y = cand["freq"].copy()
        y = (y - y.min()) / (y.max() - y.min() + 1e-9)
        return y
    # å†ä¸è¡Œå°±å…¨ 0.5ï¼ˆç­‰åŒä¸å­¸ï¼‰
    return pd.Series([0.5] * len(cand), index=cand.index)

def learn_weights_from_candidates(cand: pd.DataFrame):
    """
    åœ¨ç›®å‰å€™é¸ candï¼ˆå« slot_pop_score/content_sim/trend_zï¼‰ä¸Šï¼Œ
    ä»¥ rating_meanï¼ˆæˆ– freqï¼‰ç‚ºç›®æ¨™ï¼Œå­¸å‡ºç·šæ€§æ¬Šé‡ã€‚
    å›å‚³ (weights_dict, fitted_score_series)
    """
    feats = ["slot_pop_score", "content_sim", "trend_z"]
    df = cand.copy()
    for f in feats:
        if f not in df.columns:
            df[f] = 0.0
    X = df[feats].fillna(0.0).values
    y = _safe_target_from_cand(df).values

    # ç›¡é‡ç”¨ sklearnï¼Œæ²’æœ‰å°±èµ° numpy æœ€å°äºŒä¹˜
    try:
        from sklearn.linear_model import LinearRegression
        m = LinearRegression()
        m.fit(X, y)
        w = m.coef_
    except Exception:
        # æœ€å°äºŒä¹˜ w = (X^T X)^-1 X^T y
        XtX = X.T.dot(X) + 1e-9*np.eye(X.shape[1])
        w = np.linalg.solve(XtX, X.T.dot(y))

    # æ¬Šé‡éè²  & æ­£è¦åŒ–åˆ°åŠ ç¸½=1ï¼ˆæ˜“æ–¼è§£è®€èˆ‡å°æ¯”ï¼‰
    w = np.clip(w, 0, None)
    if w.sum() == 0:
        w = np.array([1/3, 1/3, 1/3])
    else:
        w = w / w.sum()

    weights = dict(zip(feats, [float(x) for x in w]))
    fitted = X.dot(w)
    return weights, pd.Series(fitted, index=df.index, name="ml_score")


# ================== AI æ¨¡æ“¬ç”¨ï¼šç”Ÿæˆå¡ç‰‡å…§å®¹ï¼ˆç´”æœ¬åœ°ï¼Œä¸å‘¼å« APIï¼‰ ==================
_RAND_CAST = ["è¶™éœ²æ€","å³ç£Š","ä»»å˜‰å€«","è¿ªéº—ç†±å·´","è‚–æˆ°","ç‹ä¸€åš","æ¥Šç´«","æ¥Šå†ª","èƒ¡æ­Œ","åŠ‰è©©è©©","å­«å„·","é™³æ›‰","ç¾…äº‘ç†™","è­šæ¾éŸ»","ç™½é¹¿","å¼µæ™šæ„"]
def _mock_brief(name: str, bucket: str) -> str:
    tmpl = [
        f"ã€Š{name}ã€‹è¿‘æœŸåœ¨ã€{bucket}ã€‘æ™‚æ®µè¡¨ç¾ç©©å®šï¼Œç¶²è·¯è¨è«–ä»¥è§’è‰²äººè¨­èˆ‡æƒ…ç¯€ç¯€å¥ç‚ºä¸»ï¼Œå£ç¢‘èšç„¦æ–¼æœåŒ–é“èˆ‡é…æ¨‚ã€‚",
        f"å„å¤§ç¤¾ç¾¤å°ã€Š{name}ã€‹çš„è©•åƒ¹å¤šç‚ºæ­£é¢ï¼Œè§€çœ¾é—œæ³¨é—œä¿‚ç·šæ¨é€²èˆ‡è½‰æŠ˜é»ï¼Œé©åˆé»ƒé‡‘æ™‚æ®µå›æ”¾èˆ‡å¸¶æª”ã€‚",
        f"ã€Š{name}ã€‹çš„ç²‰çµ²åœ–è­œåå‘å¥³æ€§ 18â€“44ï¼Œè¿½åŠ‡é»è‘—åº¦é«˜ï¼›åœ¨ç›¸åŒæª”æœŸå…·æ›¿ä»£æˆ–äº’è£œåƒ¹å€¼ã€‚"
    ]
    return " ".join(tmpl)

def _mock_similar_list(series_name: str, sim_candidates: list, topk=3, seed=42):
    """
    å¾ä½ ç¾æœ‰çš„ç›¸ä¼¼å‡½å¼çµæœï¼ˆæˆ– series catalogï¼‰æŒ‘å¹¾å€‹ï¼Œçµ¦å€‹ 6.8~9.2 çš„æ¨¡æ“¬è©•åˆ†ã€‚
    """
    rng = np.random.default_rng(abs(hash(series_name)) % (2**32) + seed)
    if not sim_candidates:
        # æ²’ç›¸ä¼¼å€™é¸å°±é€ å¹¾å€‹ç›¸è¿‘é¢¨æ ¼å­—ä¸²
        pool = [series_name + s for s in ["Â·å‰å‚³","Â·ç‰¹åˆ¥ç¯‡","Â·ç•ªå¤–","Â·æ–°ç·¨"]]
    else:
        pool = [s for s,_ in sim_candidates]
    rng.shuffle(pool)
    pool = pool[:topk]
    ratings = [round(float(r), 1) for r in list(rng.uniform(6.8, 9.2, size=len(pool)))]
    return list(zip(pool, ratings))

def _mock_shared_cast(topk=3, seed=99):
    rng = np.random.default_rng(seed)
    picks = rng.choice(_RAND_CAST, size=topk, replace=False)
    return [str(x) for x in picks]

def recommend(dt_str, seed_series, SLOT_POP, TREND, SDF, RDF, vec, X, catalog,
              topk=10, w_slot=0.5, w_sim=0.3, w_trend=0.2):
    t = pd.to_datetime(dt_str)
    wkd = t.weekday()
    hb  = hour_bucket_by_hour(t.hour)
    c1 = get_slot_candidates(SLOT_POP, wkd=wkd, hb=hb)
    c2 = pd.DataFrame(similar_series(seed_series, vec, X, catalog, topk=60), columns=["series","content_sim"]) if seed_series else pd.DataFrame(columns=["series","content_sim"])
    c3 = TREND.copy()
    cand = c1.merge(c2, on="series", how="outer").merge(c3, on="series", how="left").fillna(0.0)
    meta = SDF.groupby("series", as_index=False).agg(freq=("series","count"))
    if RDF is not None: meta = meta.merge(RDF, on="series", how="left")
    cand = cand.merge(meta, on="series", how="left")
    cand["score"] = w_slot*cand["slot_pop_score"] + w_sim*cand["content_sim"] + w_trend*cand["trend_z"]
    show = ["series","score","slot_pop_score","content_sim","trend_z","freq"]
    if RDF is not None: show += ["rating_mean","rating_median","rating_count"]
    return cand.sort_values("score", ascending=False).head(topk)[show].reset_index(drop=True)

# ====== Sidebarï¼šæ‡‰ç”¨æ¨¡å¼é¸æ“‡ ======
st.sidebar.title("ğŸ¯ æ„›çˆ¾é”åˆ†æå¹³å°")
app_mode = st.sidebar.selectbox(
    "é¸æ“‡åŠŸèƒ½æ¨¡å¼:",
    ["ğŸ“º åŠ‡é›†æ¨è–¦ç³»çµ±", "ğŸ”§ ç³»çµ±ç®¡ç†ä¸­å¿ƒ"],
    index=0
)

st.sidebar.markdown("---")

# å¦‚æœé¸æ“‡ç®¡ç†æ¨¡å¼ï¼Œé¡¯ç¤ºç®¡ç†å„€è¡¨æ¿
if app_mode == "ğŸ”§ ç³»çµ±ç®¡ç†ä¸­å¿ƒ":
    if ADMIN_FEATURES_AVAILABLE:
        show_admin_dashboard()
        st.stop()  # åœæ­¢åŸ·è¡Œå¾ŒçºŒçš„æ¨è–¦ç³»çµ±ä»£ç¢¼
    else:
        st.error("âŒ ç®¡ç†åŠŸèƒ½æ¨¡çµ„ä¸å¯ç”¨")
        st.info("è«‹ç¢ºä¿ admin_features.py æª”æ¡ˆå­˜åœ¨ä¸”å¯æ­£å¸¸å°å…¥")
        st.stop()

# ====== Sidebarï¼šè³‡æ–™ä¾†æº ======
st.sidebar.header("è³‡æ–™ä¾†æº")
use_upload = st.sidebar.checkbox("æ”¹ç”¨ä¸Šå‚³æª”æ¡ˆ", value=False)
if use_upload:
    up1 = st.sidebar.file_uploader("ä¸Šå‚³ç¯€ç›®è¡¨ CSVï¼ˆprogram_schedule_extracted.csvï¼‰", type=["csv"])
    up2 = st.sidebar.file_uploader("ï¼ˆå¯é¸ï¼‰ä¸Šå‚³æ”¶è¦– CSVï¼ˆintegrated_program_ratings_cleaned.csvï¼‰", type=["csv"])
    if up1:
        SDF = load_schedule(up1)
    else:
        st.stop()
    RDF = load_ratings(up2) if up2 else None
else:
    SDF = load_schedule(DEFAULT_SCHEDULE)
    RDF = load_ratings(DEFAULT_RATINGS)

# ğŸ”§ï¼ˆä½ å…ˆå‰å·²åŠ ï¼‰è£œé½Šæ¬„ä½
SDF["weekday_idx"] = pd.to_datetime(SDF["date"], errors="coerce").dt.weekday
SDF["hour_bucket"] = SDF["hour_bucket"].fillna(SDF["start_time"].apply(hour_bucket_from_time))

# ğŸ”ğŸ‘‰ åœ¨é€™è£¡è²¼ä¸Šä»¥ä¸‹ debug å€å¡Š
import os
st.write("DEBUG | CWD:", os.getcwd())
st.write("DEBUG | DEFAULT_RATINGS:", DEFAULT_RATINGS, " | exists:", os.path.exists(DEFAULT_RATINGS))

if RDF is None:
    st.error("RDF is Noneï¼šæ²’æœ‰æˆåŠŸè®€åˆ°æ”¶è¦–ç‡æª”æ¡ˆæˆ–æ¬„ä½ç„¡æ³•å°é½Š")
else:
    st.success("RDF å·²è®€åˆ°")
    st.write("DEBUG | RDF.columns:", list(RDF.columns))
    st.write("DEBUG | RDF.head():", RDF.head())

    # æª¢æŸ¥éµå€¼æ˜¯å¦å°å¾—ä¸Šï¼ˆseries äº¤é›†æœ‰å¤šå°‘ï¼‰
    series_sdf = set(SDF["series"].dropna().astype(str).str.strip().unique())
    series_rdf = set(RDF["series"].dropna().astype(str).str.strip().unique()) if "series" in RDF.columns else set()
    st.write("DEBUG | series äº¤é›†ç­†æ•¸ï¼š", len(series_sdf & series_rdf))
    # é¡¯ç¤ºå‰ 10 å€‹äº¤é›†æ¨£æœ¬
    st.write("DEBUG | series äº¤é›†æ¨£æœ¬ï¼š", list((series_sdf & series_rdf))[:10])

# ====== å»ºç´¢å¼•ï¼ˆå¿«å–ï¼‰ ======
SLOT_POP = build_slot_pop(SDF, RDF, days=60)
TREND    = build_trend(SDF, RDF)
vec, X, catalog = build_sim_index(SDF)

# ====== ä¸»é å…§å®¹ ======
st.title("æ„›çˆ¾é”ç¯€ç›®è¡¨äº’å‹•å¹³å°ï¼ˆStreamlitï¼‰")

# æ‘˜è¦
col1, col2, col3 = st.columns([1,1,1])
with col1:
    st.subheader("æ‘˜è¦ç¸½è¦½")
    total = len(SDF)
    date_min = str(SDF["date"].min().date()) if SDF["date"].notna().any() else ""
    date_max = str(SDF["date"].max().date()) if SDF["date"].notna().any() else ""
    st.metric("ç¸½ç­†æ•¸", f"{total:,}")
    st.caption(f"æ—¥æœŸç¯„åœï¼š{date_min} ~ {date_max}")
with col2:
    yy = SDF.copy()
    yy["year"] = pd.to_datetime(yy["date"], errors="coerce").dt.year
    st.subheader("å¹´ä»½è¨ˆæ•¸")
    st.dataframe(yy["year"].value_counts().sort_index().rename_axis("year").reset_index(name="count"))
with col3:
    st.subheader("æ™‚æ®µåˆ†å¸ƒ")
    st.dataframe(SDF["hour_bucket"].value_counts().sort_index().rename_axis("bucket").reset_index(name="count"))

st.divider()

# ç¯©é¸
st.subheader("ç¯€ç›®æ¸…å–®æª¢ç´¢")
c1, c2, c3, c4 = st.columns([1,1,1,2])
min_d, max_d = SDF["date"].min(), SDF["date"].max()
with c1:
    d_from = st.date_input("æ—¥æœŸï¼ˆèµ·ï¼‰", value=min_d.date() if pd.notna(min_d) else None)
with c2:
    d_to   = st.date_input("æ—¥æœŸï¼ˆè¿„ï¼‰", value=max_d.date() if pd.notna(max_d) else None)
with c3:
    bucket = st.selectbox("æ™‚æ®µ", ["(å…¨éƒ¨)","08-10","10-12","12-14","14-17","17-19","19-21","20-22","other"], index=0)
with c4:
    q = st.text_input("é—œéµå­—ï¼ˆç¯€ç›®ï¼‰", "")

df = SDF.copy()
if d_from: df = df[df["date"] >= pd.to_datetime(d_from)]
if d_to:   df = df[df["date"] <= pd.to_datetime(d_to)]
if bucket and bucket != "(å…¨éƒ¨)": df = df[df["hour_bucket"] == bucket]
if q: df = df[df["program_title"].astype(str).str.contains(re.escape(q), case=False, na=False)]
df = df.sort_values(["date","start_time"])
st.dataframe(df[["date","weekday_name","start_time","hour_bucket","program_title","source_sheet"]], use_container_width=True)

st.divider()

# æ¨è–¦
st.subheader("ç°¡æ˜“æ¨è–¦ï¼ˆSlot + ç›¸ä¼¼ + è¶¨å‹¢ï¼‰")
rc1, rc2, rc3 = st.columns([1.2,1,1])
with rc1:
    dt_str = st.text_input("ç›®æ¨™æ™‚é–“", value=f"{str(SDF['date'].max().date())} 20:00")
with rc2:
    seed = st.text_input("ç¨®å­ç¯€ç›®ï¼ˆå¯ç©ºï¼‰", "")
with rc3:
    topk = st.number_input("Top-K", min_value=3, max_value=30, value=10, step=1)

if st.button("ç”¢ç”Ÿæ¨è–¦"):
    rec = recommend(dt_str, seed if seed else None, SLOT_POP, TREND, SDF, RDF, vec, X, catalog, topk=topk)
    st.session_state['rec'] = rec  # â­ï¸ å­˜èµ·ä¾†ï¼Œçµ¦ ML/AI å€å¡Šç”¨
    st.dataframe(rec, use_container_width=True)
    st.caption("ç¸½åˆ† = 0.5*Slot + 0.3*Sim + 0.2*Trendï¼ˆå¯æ–¼ç¨‹å¼å…§èª¿æ•´ï¼‰")

st.divider()
st.header("ğŸ§ª æ·±å…¥è¨ˆç®—")

if 'rec' not in st.session_state:
    st.info("è«‹å…ˆåœ¨ä¸Šæ–¹ç”¢ç”Ÿä¸€æ¬¡æ¨è–¦ï¼ˆå–å¾—å€™é¸æ¸…å–®ï¼‰ï¼Œå†ä¾†é€™è£¡ç”¨ ML å­¸æ¬Šé‡ã€‚")
else:
    cand = st.session_state['rec'].copy()  # å–å‰›å‰›é‚£æ¬¡å€™é¸
    st.caption("èªªæ˜ï¼šåœ¨ã€ç•¶å‰å€™é¸ã€‘ä¸Šï¼Œä»¥ rating_meanï¼ˆæˆ– freqï¼‰ä½œç‚ºå­¸ç¿’ç›®æ¨™ï¼Œå­¸å‡º Slot/Sim/Trend çš„æœ€ä½³åŠ æ¬Šã€‚")

    if st.button("ç”¨æ©Ÿå™¨å­¸ç¿’è‡ªå‹•å­¸æ¬Šé‡"):
        w, ml_score = learn_weights_from_candidates(cand)
        cand["ml_score"] = ml_score
        cand2 = cand.sort_values("ml_score", ascending=False).reset_index(drop=True)

        colA, colB = st.columns(2)
        with colA:
            st.subheader("å­¸å‡ºçš„æ¬Šé‡ (æ­¸ä¸€åŒ–åˆ°ç¸½å’Œ=1)")
            st.json({
                "slot_pop_score": round(w.get("slot_pop_score",0), 3),
                "content_sim":    round(w.get("content_sim",0), 3),
                "trend_z":        round(w.get("trend_z",0), 3),
            })
        with colB:
            st.subheader("èˆ‡åŸæœ¬æ¬Šé‡çš„å°ç…§ï¼ˆåŸï¼š0.5/0.3/0.2ï¼‰")
            st.json({"original": {"slot":0.5,"sim":0.3,"trend":0.2}})

        st.subheader("ML é‡æ–°æ’åºçµæœ")
        st.dataframe(cand2, use_container_width=True)
        st.caption("æç¤ºï¼šå­¸ç¿’åƒ…æ–¼ç•¶å‰å€™é¸ä¸Šé€²è¡Œï¼Œå»ºè­°å…ˆè¼‰å…¥ ratings è®“ç›®æ¨™æ›´è²¼è¿‘çœŸå¯¦è¡¨ç¾ã€‚")

        # ä¿å­˜ä¾› AI æ¨¡æ“¬å€å¡Šä½¿ç”¨
        st.session_state['rec_ml'] = cand2

st.divider()
st.header("ğŸ¤– AI åŠŸèƒ½æ¨¡æ“¬å€å¡Š")

if 'rec' not in st.session_state and 'rec_ml' not in st.session_state:
    st.info("è«‹å…ˆåœ¨ä¸Šæ–¹ç”¢ç”Ÿæ¨è–¦ï¼Œæˆ–åœ¨ã€æ©Ÿå™¨å­¸ç¿’å€å¡Šã€‘é‡æ–°æ’åºå¾Œå†ä¾†æ¨¡æ“¬ã€‚")
else:
    base = st.session_state.get('rec_ml', st.session_state.get('rec')).copy()
    N = st.slider("è¦ç”Ÿæˆå¹¾å€‹ AI æ¨è–¦å¡ç‰‡", min_value=3, max_value=15, value=6, step=1)
    bucket_hint = st.selectbox("æƒ…å¢ƒï¼ˆåƒ…æ–‡å­—ï¼Œç”¨æ–¼æ‘˜è¦æ¨¡æ“¬ï¼‰", 
                               ["19-21 é»ƒé‡‘å¸¶", "20-22 é»ƒé‡‘å¸¶", "10-12 ä¸Šåˆå¸¶", "å…¶ä»–æ™‚æ®µ"], index=0)
    if st.button("ç”Ÿæˆ AI æ¨¡æ“¬å¡ç‰‡"):
        # å˜—è©¦æ‹¿ç›¸ä¼¼ç¯€ç›®ï¼ˆè‹¥ä½ æœ‰ vec/X/catalogï¼‰
        sim_index = {}
        try:
            for s in base["series"].head(N):
                sim_index[s] = similar_series(s, vec, X, catalog, topk=10)
        except Exception:
            pass

        for i, row in base.head(N).iterrows():
            series = str(row["series"])
            # æ¨¡æ“¬å…§å®¹
            brief = _mock_brief(series, bucket_hint)
            sim_list = _mock_similar_list(series, sim_index.get(series, []), topk=3, seed=2025+i)
            cast = _mock_shared_cast(topk=3, seed=1000+i)

            with st.expander(f"ã€Š{series}ã€‹ï½œAI æ¨¡æ“¬è³‡è¨Šå¡"):
                c1, c2 = st.columns([2,1])
                with c1:
                    st.markdown(f"**ç¶²è·¯æ‘˜è¦ï¼ˆæ¨¡æ“¬ï¼‰**ï¼š{brief}")
                    st.markdown("**é¡ä¼¼å½±è¦–åŠ‡ï¼‹è©•åˆ†ï¼ˆæ¨¡æ“¬ï¼‰**ï¼š")
                    st.write(pd.DataFrame(sim_list, columns=["title", "mock_score"]))
                with c2:
                    st.metric("Slot", f"{row.get('slot_pop_score',0):.3f}")
                    st.metric("Sim",  f"{row.get('content_sim',0):.3f}")
                    st.metric("Trend",f"{row.get('trend_z',0):.3f}")
                    if 'rating_mean' in row:
                        st.metric("Rating(å‡å€¼)", f"{row.get('rating_mean',float('nan')):.3f}" if pd.notna(row.get('rating_mean')) else "-")
                    st.caption(f"å¯èƒ½å…±åŒä¸»è§’ï¼ˆæ¨¡æ“¬ï¼‰ï¼š{', '.join(cast)}")
        st.success("å·²ç”Ÿæˆ AI æ¨¡æ“¬å¡ç‰‡ï¼ˆç´”æœ¬åœ°éš¨æ©Ÿ/è¦å‰‡ï¼Œç„¡å¤–éƒ¨å‘¼å«ï¼‰ã€‚")
# =========================
# ğŸ¤– AI æ¨è–¦å¾ŒçºŒé«”é©—ï¼ˆæ¨¡æ“¬ç‰ˆï¼‰
# æ¨è–¦ç†ç”± + é¡ä¼¼åŠ‡å¡ç‰‡ï¼ˆé»‘é¡/ä¸‰é«”/ç«Šè½é¢¨é›²ï¼‰ + è§€çœ¾è©•è«–æ‘˜è¦
# ä¾è³´ï¼šst.session_state['rec_ml'] æˆ– ['rec'] ä½œç‚ºä¾†æºå€™é¸
# åœ–ç‰‡è«‹æ”¾åœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„ï¼šé»‘é¡.webpã€ä¸‰é«”.jpgã€ç«Šè½é¢¨é›².webp
# =========================
st.divider()
st.header("ğŸ¤– AI æ¨è–¦å¾ŒçºŒé«”é©—")

# å–ç”¨ä¸Šä¸€æ®µæ¨è–¦çµæœ
_base_rec = st.session_state.get('rec_ml', st.session_state.get('rec'))
if _base_rec is None or len(_base_rec) == 0:
    st.info("è«‹å…ˆåœ¨ä¸Šæ–¹ç”¢ç”Ÿä¸€æ¬¡æ¨è–¦ï¼Œæˆ–ä½¿ç”¨ã€æ©Ÿå™¨å­¸ç¿’å€å¡Šã€é‡æ–°æ’åºå¾Œå†å›ä¾†ã€‚")
else:
    # è®“ä½¿ç”¨è€…æŒ‘ä¸€ç­†æ¨è–¦ä½œç‚ºã€Œä¸»æ¨è–¦å°è±¡ã€
    options = _base_rec["series"].astype(str).tolist()
    sel = st.selectbox("é¸æ“‡ä¸€å€‹ä¸»æ¨è–¦ç¯€ç›®ï¼ˆä½œç‚ºç†ç”±èˆ‡å»¶ä¼¸è³‡è¨Šçš„æ ¸å¿ƒï¼‰", options, index=0)

    # æ‰¾åˆ°è©²åˆ—ï¼ˆå–åˆ†æ•¸ä¾›ç†ç”±ç”Ÿæˆï¼‰
    row = _base_rec[_base_rec["series"].astype(str) == sel].head(1)
    slot_v  = float(row["slot_pop_score"].iloc[0]) if "slot_pop_score" in row and not row["slot_pop_score"].isna().iloc[0] else 0.0
    sim_v   = float(row["content_sim"].iloc[0])   if "content_sim" in row and not row["content_sim"].isna().iloc[0]   else 0.0
    trend_v = float(row["trend_z"].iloc[0])       if "trend_z" in row and not row["trend_z"].isna().iloc[0]           else 0.0
    rating_mean = row["rating_mean"].iloc[0] if "rating_mean" in row else None

    # â€”â€” æ¨¡æ“¬æ¨è–¦ç†ç”±ï¼ˆä¾ä½ çš„ä¸‰åˆ†é …ç”Ÿæˆè‡ªç„¶èªå¥ï¼‰â€”â€”
    reason_bits = []
    # Slot
    if slot_v >= 0.75:
        reason_bits.append("åœ¨è¿‘ 60 å¤©çš„ç›¸åŒæ™‚æ®µä¸­è¡¨ç¾å±¬æ–¼å‰ 25%")
    elif slot_v >= 0.5:
        reason_bits.append("åœ¨åŒæ™‚æ®µè¡¨ç¾ç©©å®šï¼Œä½æ–¼ä¸­ä¸Šå€é–“")
    else:
        reason_bits.append("è¿‘ 60 å¤©åŒæ™‚æ®µæ›å…‰é€æ­¥å¢åŠ ")
    # Sim
    if sim_v > 0:
        reason_bits.append("èˆ‡ä½ çš„ç¨®å­åå¥½é«˜åº¦ç›¸ä¼¼")
    # Trend
    if trend_v > 0.3:
        reason_bits.append("è¿‘æœŸè¨è«–åº¦/ç†±åº¦æ˜é¡¯ä¸Šå‡")
    elif trend_v > 0:
        reason_bits.append("è¿‘æœŸç†±åº¦ç•¥æœ‰å¢é•·")
    else:
        reason_bits.append("ç†±åº¦å¹³ç©©å¯ä½œç‚ºæª”æœŸç©©å®šé¸é …")
    # rating
    if rating_mean is not None and pd.notna(rating_mean):
        try:
            reason_bits.append(f"å¹³å‡æ”¶è¦–ç´„ {float(rating_mean):.2f}")
        except:
            pass

    st.subheader("æ¨è–¦ç†ç”±")
    st.markdown(f"**ç‚ºä»€éº¼æ¨è–¦ã€Š{sel}ã€‹ï¼Ÿ** " + "ï¼›".join(reason_bits) + "ã€‚")

    # â€”â€” é¡ä¼¼åŠ‡å¡ç‰‡ï¼ˆå›ºå®šä¸‰å¼µï¼‰ï¼‹ è©•åˆ†ï¼ˆæ¨¡æ“¬ï¼‰ï¼‹ åœ–ç‰‡ â€”â€” 
    st.subheader("é¡ä¼¼å½±è¦–åŠ‡")
    DEFAULT_IMG_DIR = DEFAULT_DIR  # ä½ å°ˆæ¡ˆä¸€é–‹å§‹å°±å®šç¾©çš„ DEFAULT_DIR
    cards = [
        {"title": "é»‘é¡", "file": "é»‘é¡.webp", "desc": "è‹±åœ‹ç§‘å¹»é¸é›†ï¼Œèšç„¦ç§‘æŠ€èˆ‡äººæ€§çš„é»‘æš—é¢ï¼Œå–®å…ƒåŠ‡å½¢å¼ï¼Œæ¯é›†ç¨ç«‹ã€‚"},
        {"title": "ä¸‰é«”", "file": "ä¸‰é«”.jpg", "desc": "ç¡¬ç§‘å¹»æ•˜äº‹ï¼Œåœç¹æ–‡æ˜å­˜äº¡èˆ‡å®‡å®™ç§©åºçš„å®å¤§å‘½é¡Œï¼Œç†æ€§æ´¾å—çœ¾åå¥½ã€‚"},
        {"title": "ç«Šè½é¢¨é›²", "file": "ç«Šè½é¢¨é›².webp", "desc": "é¦™æ¸¯çŠ¯ç½ªé¡å‹ç‰‡ï¼Œæƒ…å ±åšå¼ˆèˆ‡å•†æˆ°å…ƒç´ å¼·ï¼Œæƒ…å ±å°çµ„è¦–è§’æ¨é€²ã€‚"},
    ]
    # çµ¦å€‹æ¨¡æ“¬è©•åˆ†ï¼ˆå›ºå®š/è¼•éš¨æ©Ÿçš†å¯ï¼›é€™è£¡çµ¦å›ºå®šå€é–“ï¼‰
    mock_scores = {"é»‘é¡": 8.8, "ä¸‰é«”": 8.3, "ç«Šè½é¢¨é›²": 7.9}

    c1, c2, c3 = st.columns(3)
    for c, item in zip([c1, c2, c3], cards):
        with c:
            img_path = os.path.join(DEFAULT_IMG_DIR, item["file"])
            if os.path.exists(img_path):
                st.image(img_path, use_container_width=True)
            else:
                st.caption(f"ï¼ˆæ‰¾ä¸åˆ°åœ–ç‰‡æª”ï¼š{item['file']}ï¼Œè«‹æ”¾åœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„ï¼‰")
            st.markdown(f"**{item['title']}**")
            st.caption(item["desc"])
            st.metric("æ¨¡æ“¬è©•åˆ†", f"{mock_scores.get(item['title'], 8.0):.1f} / 10")

    # â€”â€” è§€çœ¾è©•è«–æ‘˜è¦ï¼ˆæ¨¡æ“¬ï¼‰â€”â€”
    st.subheader("è§€çœ¾è©•è«–æ‘˜è¦ï¼ˆæ¨¡æ“¬ï¼‰")
    # å¯ä¾ç•¶ä¸‹ä¸»æ¨è–¦ã€æˆ–æ‰€é¸æ™‚æ®µæƒ…å¢ƒæ‹¼æ¥ä¸åŒçš„èªå¥
    demo_comments = [
        f"ã€Š{sel}ã€‹çš„ç¯€å¥åœ¨ä¸­å¾Œæ®µæ˜é¡¯åŠ å¿«ï¼Œè§’è‰²é—œä¿‚ç·šæ›´ç·Šå¯†ï¼Œè¨è«–å¤šé›†ä¸­åœ¨é—œéµè½‰æŠ˜èˆ‡çµå±€é‹ªé™³ã€‚",
        "æœåŒ–é“èˆ‡å ´æ™¯è³ªæ„Ÿç²å¾—æ™®éå¥½è©•ï¼›å°‘æ•¸è§€çœ¾å°éƒ¨åˆ†æ”¯ç·šç¯‡å¹…æœ‰åˆ†æ­§ã€‚",
        "å¦‚æœå–œæ­¡ä¸–ç•Œè§€èˆ‡è­°é¡Œæ€è¾¨ï¼Œé¡ä¼¼çš„ã€Šé»‘é¡ã€‹ã€Šä¸‰é«”ã€‹èƒ½æä¾›æ›´å¼·çš„æ¦‚å¿µå¯†åº¦ï¼›æƒ³è¦åé¡å‹å¿«æ„Ÿå¯é¸ã€Šç«Šè½é¢¨é›²ã€‹ã€‚",
    ]
    st.write("- " + "\n- ".join(demo_comments))

    st.caption("ä»¥ä¸Šç‚ºä»‹é¢ç¤ºæ„ï¼ˆä¸å‘¼å«å¤–éƒ¨ APIï¼‰ã€‚æœªä¾†è¦æ¥çœŸè³‡æ–™ï¼šåªè¦æŠŠã€ç†ç”±å¥ã€èˆ‡ã€å¡ç‰‡è³‡è¨Š/è©•åˆ†ã€å¾ API/è³‡æ–™åº«å¡«å…¥å³å¯ã€‚")